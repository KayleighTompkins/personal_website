[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I’m Kayleigh Tompkins - a researcher and former policy advisor (UK Government) focused on the intersection of international relations and institutional stability. I’m currently pursuing graduate studies in Public Policy at the University of Texas at Dallas (UTD)."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Welcome!",
    "section": "Research Interests",
    "text": "Research Interests\nMy work focuses on why states leave international organizations and the challenges of maintaining international cooperation. I’m currently interested in:\n\nInternational Organizations\nDemocratic Backsliding\nPolitical Economy of Climate Change"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I began my academic journey in the UK with a Bachelor’s degree in History from the University of Oxford, St Anne’s College. Graduating just seven days after the UK’s vote to leave the EU in June 2016, I spent the next five years working for the UK government designing new policies which defined the UK’s approach to international trade disputes after Brexit.\nAfter a family relocation to the US in 2022, I helped lead UC Berkeley’s strategic engagement with its most important global and domestic partners. I re-embraced academia after moving to Dallas in 2024, beginning my graduate studies which are focused on the same question that inspired my career: why do states leave international organizations and how do those organizations respond to such exits?\n\nWhen not walking the halls of Green Hall, I enjoy hiking, running, team sports and hanging out with my husband and our two cats - Nellie and Norman who like to “help” with my research and coding."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Hello, Goodbye, Pay Later: Why States Withhold Funds from International Organizations. Working Paper\n\nAn extension of von Borzyskowski and Vabulas (2019), this paper examines the financial dimensions of state commitment and withdrawal from international organizations. \n\nSearching for Democracy: Citizen Perceptions of Democratic Backsliding with Samiul Haque.\n\nWe combine Google Trends data with the Democratic Erosion Events Database to identify if citizens can recognize and respond to incidences of democratic backsliding. It is scheduled to be presented at the Midwest Political Science Association (MPSA) in April 2026.\n\n\nSliding in the Polls: The Impact of Hurricane Helene on Voter Behavior in North Carolina with Mamie Cincotta.\n\nWe leverage landslide data and predicted hurricane path maps to investigate the impact of Hurricane Helene on voter turnout and behavior in the 2024 Presidential elections held in North Carolina. It is due to be presented at the Midwest Political Science Association (MPSA) in April 2026."
  },
  {
    "objectID": "research.html#current-projects",
    "href": "research.html#current-projects",
    "title": "Research",
    "section": "",
    "text": "Hello, Goodbye, Pay Later: Why States Withhold Funds from International Organizations. Working Paper\n\nAn extension of von Borzyskowski and Vabulas (2019), this paper examines the financial dimensions of state commitment and withdrawal from international organizations. \n\nSearching for Democracy: Citizen Perceptions of Democratic Backsliding with Samiul Haque.\n\nWe combine Google Trends data with the Democratic Erosion Events Database to identify if citizens can recognize and respond to incidences of democratic backsliding. It is scheduled to be presented at the Midwest Political Science Association (MPSA) in April 2026.\n\n\nSliding in the Polls: The Impact of Hurricane Helene on Voter Behavior in North Carolina with Mamie Cincotta.\n\nWe leverage landslide data and predicted hurricane path maps to investigate the impact of Hurricane Helene on voter turnout and behavior in the 2024 Presidential elections held in North Carolina. It is due to be presented at the Midwest Political Science Association (MPSA) in April 2026."
  },
  {
    "objectID": "research.html#academic-publications",
    "href": "research.html#academic-publications",
    "title": "Research",
    "section": "Academic Publications",
    "text": "Academic Publications\n\nPeinhardt, C. & Tompkins, K. (Under Review). Review of Exit from International Organizations, Costly Negotiation for Institutional Change. Perspectives on Politics."
  },
  {
    "objectID": "research.html#policy-reports",
    "href": "research.html#policy-reports",
    "title": "Research",
    "section": "Policy Reports",
    "text": "Policy Reports\n\nTompkins, K., Lentini, F., & Adam, E. (2025, August). The US-UK Economic Propsperity Deal: An Analysis from BritishAmerican Business. https://www.babinc.org/wp-content/uploads/The-US-UK-Economic-Prosperity-Deal-An-Analysis-from-BAB.pdf. BritishAmerican Business."
  },
  {
    "objectID": "Assignments.html",
    "href": "Assignments.html",
    "title": "Knowledge Mining",
    "section": "",
    "text": "In his provocative 2001 article “Statistical modeling: The two cultures”, Leo Breiman describes two opposing cultures in statistical sciences: one focused on using data to find causal parameters and identify causal relationships (“data modelling”) and the other which treats data mechanisms as a “black box” which forgoes causal understanding in favor of accurate predictions “algorithmic modeling” - now more commonly known as machine learning.\nBreiman critiques adherents of the data modeling approach for focusing too greatly on proving theorems that don’t capture real world intricacies, as well as the vulnerability of data modeling approachs to the strength of the assumptions made about their data (e.g. how it is distributed). He pointedly accuses his fellow statisticians of ignoring advances in algorithmic modeling which are able to solve complex challenges.\nDerided upon publication, Breiman radically challenged his peers to change the question that they explored, shifting from “why did this happen” to “what will happen next”. While this was a foundational critique of the field, his examples were mostly based on diagnosing and predicting medical illnesses, which do seem well suited to predictive approaches. However, while it is of course, useful, to predict who will get cancer, researchers could never conceive of abandoning research into what causes cancer.\nIn “To Explain or to Predict”, Galit Shmueli made several important contributions to this academic debate. Interestingly, she explains why a good explanatory model can be a bad predictor due overfitting the specific sample which increases variance when applied to new data - the Bias-Variance Trade-off. Predictive models, in contrast, prioritize minimizing a combination of bias and variance, and accept a little bias to reduce that variance, meaning that the addition of a “wrong” variable can make prediction more stable.\nShe outlines several useful uses of prediction to enhance explanatory work, including hypothesis generation, determining which way of measuring a concept is the most appropriate, improving explanations and theory comparison. She recommends using predictive models as a ceiling, first running the black box algorithm to see the maximum possible accuracy and then run the theory-based model. The gap (or lack of) between the two models generates important feedback as to whether the theory captures the phenomenon a researcher is trying to measure.\nTowards a third culture\nSince Breiman and Shmueli’s papers, a potential “third culture” has emerged which marries the strengths of each school of thought. This approach - Causal Machine Learning - uses predictive algorithms inside explanatory methods, combining the accuracy benefits of the algorithmic modeling culture to strengthen the scientific claims of the data modeling culture. For example, methods like Synthetic Control have a goal which is firmly rooted in explanatory, causal inference, but now employ predictive algorithms as the means to optimize the method - e.g. using Random Forests to perfectly weight the “control states” which serves as the counterfactual to the treated unit."
  },
  {
    "objectID": "index.html#professional-background",
    "href": "index.html#professional-background",
    "title": "Welcome!",
    "section": "Professional Background",
    "text": "Professional Background\nBefore returning to academia, I served as a Policy Advisor for the UK Government. This experience shapes my research, bridging the gap between theoretical political economy and practitioner insights.\nI recently served as a Policy Fellow with BritishAmerican Business where I analyzed transatlantic trade policy and published an analysis of the UK-US Economic Prosperity Deal."
  },
  {
    "objectID": "Assignments.html#assignment-1",
    "href": "Assignments.html#assignment-1",
    "title": "Knowledge Mining",
    "section": "Assignment 1",
    "text": "Assignment 1\n\nLab 1: R Programming Basics I\nLab 2: R Programming Basics II"
  },
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do? A: Counts how many elements are in an object. \n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  A: I'm not sure, it cleared my environment but so did rm(x,y)\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=F) # What about byrow=F? A: This tells R to fill the matrix by putting 1 and 2 in the first column and then 3 and 4 in the second. In byrow=T, R fills the letters by row. \n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nsqrt(x) # What does x look like? A: It took the square root of every element in the matrix.\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate? A: It generates y b y adding x to a vector of 50 random numbers which have a mean of 50 and a standard distribution of 0.1. \n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9959441\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab01.html#create-object-using-the-assignment-operator--",
    "href": "Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "Lab01.html#using-function",
    "href": "Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do? A: Counts how many elements are in an object. \n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "Lab01.html#using---operators",
    "href": "Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  A: I'm not sure, it cleared my environment but so did rm(x,y)"
  },
  {
    "objectID": "Lab01.html#matrix-operations",
    "href": "Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=F) # What about byrow=F? A: This tells R to fill the matrix by putting 1 and 2 in the first column and then 3 and 4 in the second. In byrow=T, R fills the letters by row. \n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nsqrt(x) # What does x look like? A: It took the square root of every element in the matrix.\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate? A: It generates y b y adding x to a vector of 50 random numbers which have a mean of 50 and a standard distribution of 0.1. \n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9959441\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "Lab01.html#simple-descriptive-statistics-base",
    "href": "Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab02.html",
    "href": "Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do? A: Tells R to drop certain columns or rows. \n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nlibrary(MASS)\n\nWarning: package 'MASS' was built under R version 4.4.1\n\nlibrary(ISLR)\n\nWarning: package 'ISLR' was built under R version 4.4.1\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the difference between \"confidence\" and \"prediction\" difference? A: The prediction intervals are wider than the confidence intervals. \n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.4.1\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.4.1\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nProblems with the dataset\n\n\n# Define the URL\nurl &lt;- \"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\"\n\n# Create a temporary file path\ntmp &lt;- tempfile(fileext = \".dta\")\n\n# Download the file to that path\ndownload.file(url, tmp, mode = \"wb\")\n\n# Read the file using haven\nlibrary(haven)\nTEDS_2016 &lt;- read_dta(tmp)\n\n#Check structure\nstr(TEDS_2016$Tondu)\n\n dbl+lbl [1:1690] 3, 5, 3, 5, 9, 4, 9, 6, 9, 9, 5, 5, 9, 5, 4, 9, 9, 4, 3, ...\n @ label       : chr \"Position on unification and independence\"\n @ format.stata: chr \"%10.0g\"\n @ labels      : Named num [1:7] 1 2 3 4 5 6 9\n  ..- attr(*, \"names\")= chr [1:7] \"Immediate unification\" \"Maintain the status quo,move toward unification\" \"Maintain the status quo, decide either unification or independence\" \"Maintain the status quo forever\" ...\n\n# Look first few columns\nsummary(TEDS_2016[, 1:5])\n\n    District         Sex             Age           Edu            Arear      \n Min.   : 201   Min.   :1.000   Min.   :1.0   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1401   1st Qu.:1.000   1st Qu.:2.0   1st Qu.:2.000   1st Qu.:1.000  \n Median :6406   Median :1.000   Median :3.0   Median :3.000   Median :3.000  \n Mean   :4661   Mean   :1.486   Mean   :3.3   Mean   :3.334   Mean   :2.744  \n 3rd Qu.:6604   3rd Qu.:2.000   3rd Qu.:5.0   3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :6806   Max.   :2.000   Max.   :5.0   Max.   :9.000   Max.   :6.000  \n\n# View the labels associated with values\nlibrary(labelled)\n\nWarning: package 'labelled' was built under R version 4.4.1\n\nval_labels(TEDS_2016$Tondu)\n\n                                             Immediate unification \n                                                                 1 \n                   Maintain the status quo,move toward unification \n                                                                 2 \nMaintain the status quo, decide either unification or independence \n                                                                 3 \n                                   Maintain the status quo forever \n                                                                 4 \n                  Maintain the status quo,move toward independence \n                                                                 5 \n                                            Immediate independence \n                                                                 6 \n                                                       Nonresponse \n                                                                 9 \n\n#Column Names\ncolnames(TEDS_2016)\n\n [1] \"District\"        \"Sex\"             \"Age\"             \"Edu\"            \n [5] \"Arear\"           \"Career\"          \"Career8\"         \"Ethnic\"         \n [9] \"Party\"           \"PartyID\"         \"Tondu\"           \"Tondu3\"         \n[13] \"nI2\"             \"votetsai\"        \"green\"           \"votetsai_nm\"    \n[17] \"votetsai_all\"    \"Independence\"    \"Unification\"     \"sq\"             \n[21] \"Taiwanese\"       \"edu\"             \"female\"          \"whitecollar\"    \n[25] \"lowincome\"       \"income\"          \"income_nm\"       \"age\"            \n[29] \"KMT\"             \"DPP\"             \"npp\"             \"noparty\"        \n[33] \"pfp\"             \"South\"           \"north\"           \"Minnan_father\"  \n[37] \"Mainland_father\" \"Econ_worse\"      \"Inequality\"      \"inequality5\"    \n[41] \"econworse5\"      \"Govt_for_public\" \"pubwelf5\"        \"Govt_dont_care\" \n[45] \"highincome\"      \"votekmt\"         \"votekmt_nm\"      \"Blue\"           \n[49] \"Green\"           \"No_Party\"        \"voteblue\"        \"voteblue_nm\"    \n[53] \"votedpp_1\"       \"votekmt_1\"      \n\n\nThe dataset has a few issues including: - Nonresponses to survey questions were assigned a value of 9 which will artificially inflate means etc.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nTEDS_clean &lt;- TEDS_2016 %&gt;%\n  mutate(across(everything(), ~na_if(., 9))) %&gt;%\n  mutate(across(everything(), ~na_if(., 95))) %&gt;%\n  mutate(across(everything(), ~na_if(., 98)))\n\n-Variable labels should be recoded as a factor\n\nTEDS_ready &lt;- haven::as_factor(TEDS_clean)\n\n#Check it worked\nsummary(TEDS_clean$Tondu)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   3.000   4.000   3.751   5.000   6.000     121 \n\n\n\nHow to deal with missing values\n\nI’ve already recoded non-responses, originally recorded as 9 above. Next I’ll check for other nas in the original dataset (before I cleaned the nonresponses coded as 9) and get the dataset to record na if there are values of . or 9.\n\ncolSums(is.na(TEDS_2016))\n\n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2        votetsai           green \n              0               0               0             429               0 \n    votetsai_nm    votetsai_all    Independence     Unification              sq \n            429             248               0               0               0 \n      Taiwanese             edu          female     whitecollar       lowincome \n              0              10               0               0               0 \n         income       income_nm             age             KMT             DPP \n              0             330               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care      highincome \n              0               0               0               0             330 \n        votekmt      votekmt_nm            Blue           Green        No_Party \n              0             429               0               0               0 \n       voteblue     voteblue_nm       votedpp_1       votekmt_1 \n              0             429             187             187 \n\nlibrary(dplyr)\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  mutate(across(everything(), ~na_if(., 9)))\n\n\nExplore the relationship between Tondu and other variables including female, DPP, age, income, edu, Taiwanese and Econ_worse. What methods would you use?\n\nFirst I would check the nature of the variables - are they continuous or binary?\nThen I would visualize relationships and check correlations/regressions.\n\n# Check variable types\nTEDS_2016 %&gt;% \n  select(Tondu, female, DPP, age, income, edu, Taiwanese, Econ_worse) %&gt;% \n  str()\n\ntibble [1,690 × 8] (S3: tbl_df/tbl/data.frame)\n $ Tondu     : dbl+lbl [1:1690]  3,  5,  3,  5, NA,  4, NA,  6, NA, NA,  5,  5, NA,  ...\n   ..@ label       : chr \"Position on unification and independence\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"Immediate unification\" \"Maintain the status quo,move toward unification\" \"Maintain the status quo, decide either unification or independence\" \"Maintain the status quo forever\" ...\n $ female    : num [1:1690] 1 1 0 0 1 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ DPP       : num [1:1690] 0 0 0 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ age       : num [1:1690] 59 39 63 55 76 64 75 54 64 59 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income    : num [1:1690] 8 7 8 5 5.5 NA 1 10 2 5.5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ edu       : num [1:1690] 4 5 5 2 1 2 1 5 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Taiwanese : num [1:1690] 1 0 0 1 0 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Econ_worse: num [1:1690] 0 0 1 1 0 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n\n\nTondu is an ordinal variable coded from Immediate Unification through to Immediate Independence. I will recode it into numbers where immediate unification = 1 and immediate independence = 6.\n\nlibrary(dplyr)\n\nTEDS_ready &lt;- TEDS_ready %&gt;%\n  mutate(Tondu_numeric = case_when(\n    as_factor(Tondu) == \"Immediate unification\" ~ 1,\n    as_factor(Tondu) == \"Maintain the status quo,move toward unification\" ~ 2,\n    as_factor(Tondu) == \"Maintain the status quo, decide either unification or independence\" ~ 3,\n    as_factor(Tondu) == \"Maintain the status quo forever\" ~ 4,\n    as_factor(Tondu) == \"Maintain the status quo,move toward independence\" ~ 5,\n    as_factor(Tondu) == \"Immediate independence\" ~ 6,\n    TRUE ~ NA_real_  # This handles the \"Nonresponse\" or NA values\n  ))\n\nFemale is a binary variable coded 1 if respondent is female. Likewise DPP, Taiwanese and Econ_worse are also binary and likely coded 1 if the respondent supports the DPP, is Taiwanese and thinks the economy is worse.\nAge is a ratio variable and income is likely a coded scale from 1-10.\nDescriptive statistics\n\nTEDS_summary &lt;- TEDS_2016 %&gt;%\n  group_by(as_factor(Tondu)) %&gt;%\n  summarize(across(\n    c(female, DPP, age, income, edu, Taiwanese, Econ_worse), \n    ~mean(.x, na.rm = TRUE)\n  ))\nprint(TEDS_summary)\n\n# A tibble: 7 × 8\n  `as_factor(Tondu)`       female    DPP   age income   edu Taiwanese Econ_worse\n  &lt;fct&gt;                     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Immediate unification     0.519 0.0370  57.9   4.02  2.31     0.407      0.481\n2 Maintain the status quo…  0.344 0.183   55.7   5.21  3.25     0.267      0.567\n3 Maintain the status quo…  0.460 0.308   45.8   5.35  3.58     0.553      0.562\n4 Maintain the status quo…  0.540 0.220   52.8   4.94  3.03     0.585      0.521\n5 Maintain the status quo…  0.458 0.621   42.4   5.39  3.83     0.871      0.592\n6 Immediate independence    0.5   0.648   48.0   4.41  2.86     0.963      0.639\n7 &lt;NA&gt;                      0.744 0.0909  64.5   3.95  1.72     0.595      0.413\n\n\nVisualizations\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'Auto':\n\n    mpg\n\nlibrary(dplyr)\n\n#For categorical variables - female, DPP, Econ_Worse, Taiwanese:\n\n#Female\nTEDS_2016 %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(female), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#DPP\nTEDS_2016 %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(DPP), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#Econ_worse\nTEDS_2016 %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(Econ_worse), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#Taiwanese\nTEDS_2016 %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(Taiwanese), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#For continuous or ordinal - age, education, income\n# Age Plot\nggplot(TEDS_2016, aes(x = as_factor(Tondu), y = age)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n#Edu\n# Simple Age Plot\nggplot(TEDS_2016, aes(x = as_factor(Tondu), y = edu)) +\n  geom_boxplot() +\n  coord_flip()\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n#Income\n# Simple Age Plot\nggplot(TEDS_2016, aes(x = as_factor(Tondu), y = income)) +\n  geom_boxplot() +\n  coord_flip()\n\nWarning: Removed 107 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nCorrelations/regressions\n\nmodel1 &lt;- lm(Tondu_numeric ~ female + DPP + age + edu + Taiwanese + Econ_worse, data=TEDS_ready)\n\nsummary(model1)\n\n\nCall:\nlm(formula = Tondu_numeric ~ female + DPP + age + edu + Taiwanese + \n    Econ_worse, data = TEDS_ready)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8950 -0.8962 -0.0027  0.8334  3.1994 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.548854   0.176269  20.133  &lt; 2e-16 ***\nfemale       0.057622   0.054247   1.062    0.288    \nDPP          0.524508   0.059802   8.771  &lt; 2e-16 ***\nage         -0.009334   0.002091  -4.464 8.63e-06 ***\nedu         -0.008611   0.023483  -0.367    0.714    \nTaiwanese    0.757640   0.060766  12.468  &lt; 2e-16 ***\nEcon_worse  -0.031005   0.055590  -0.558    0.577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.06 on 1555 degrees of freedom\n  (128 observations deleted due to missingness)\nMultiple R-squared:  0.2098,    Adjusted R-squared:  0.2067 \nF-statistic: 68.79 on 6 and 1555 DF,  p-value: &lt; 2.2e-16\n\n# Visualize\nlibrary(sjPlot)\n\nWarning: package 'sjPlot' was built under R version 4.4.1\n\n# Visualize the coefficients\nplot_model(model1, \n           show.values = TRUE, \n           value.offset = .3,\n           title = \"Predictors of Tondu Position (Unification to Independence)\")\n\n\n\n\n\n\n\n\nThis regression plot and visualization indicates that supporting the DPP and being Taiwanese were statistically significant predictors of supporting Taiwanese independence. Being older was associated with a statistically significant decrease in support for independence, although this was a very small effect.\n\nHow about the votetsai variable (vote for DPP candidate Tsai Ing-wen)?\n\nThis suggests that voters who voted for Tsai Ing-wen were more supportive of independence and moving towards independence than voters who didn’t.\n\n# Check the labels first\nlibrary(labelled)\nval_labels(TEDS_2016$votetsai)\n\nNULL\n\n# Recode if necessary (assuming 1 is Tsai, and 0 or other numbers are missing)\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  mutate(votetsai = na_if(votetsai, 9)) \n\nTEDS_ready %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(votetsai), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nGenerate frequency table and barchart of the Tondu variable. Assign labels to the variable using the following:\n\n\n\nTEDS_2016$Tondu&lt;-as.numeric(TEDS_2016$Tondu,labels=c(\"Unification now”, “Status quo, unif. in future”, “Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now”, “No response\"))\n\n# Standard frequency table\ntable(TEDS_2016$Tondu)\n\n\n  1   2   3   4   5   6 \n 27 180 546 328 380 108 \n\n# Percentages (better for policy reports)\nprop.table(table(TEDS_2016$Tondu)) * 100\n\n\n        1         2         3         4         5         6 \n 1.720841 11.472275 34.799235 20.905035 24.219248  6.883365"
  },
  {
    "objectID": "Lab02.html#indexing-data-using",
    "href": "Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do? A: Tells R to drop certain columns or rows. \n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "Lab02.html#loading-data-from-github-remote",
    "href": "Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "Lab02.html#load-data-from-islr-website",
    "href": "Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "Lab02.html#linear-regression",
    "href": "Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "library(MASS)\n\nWarning: package 'MASS' was built under R version 4.4.1\n\nlibrary(ISLR)\n\nWarning: package 'ISLR' was built under R version 4.4.1\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the difference between \"confidence\" and \"prediction\" difference? A: The prediction intervals are wider than the confidence intervals. \n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Lab02.html#multiple-linear-regression",
    "href": "Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.4.1\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.4.1\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#qualitative-predictors",
    "href": "Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Assignments.html#reflection-1",
    "href": "Assignments.html#reflection-1",
    "title": "Knowledge Mining",
    "section": "",
    "text": "In his provocative 2001 article “Statistical modeling: The two cultures”, Leo Breiman describes two opposing cultures in statistical sciences: one focused on using data to find causal parameters and identify causal relationships (“data modelling”) and the other which treats data mechanisms as a “black box” which forgoes causal understanding in favor of accurate predictions “algorithmic modeling” - now more commonly known as machine learning.\nBreiman critiques adherents of the data modeling approach for focusing too greatly on proving theorems that don’t capture real world intricacies, as well as the vulnerability of data modeling approachs to the strength of the assumptions made about their data (e.g. how it is distributed). He pointedly accuses his fellow statisticians of ignoring advances in algorithmic modeling which are able to solve complex challenges.\nDerided upon publication, Breiman radically challenged his peers to change the question that they explored, shifting from “why did this happen” to “what will happen next”. While this was a foundational critique of the field, his examples were mostly based on diagnosing and predicting medical illnesses, which do seem well suited to predictive approaches. However, while it is of course, useful, to predict who will get cancer, researchers could never conceive of abandoning research into what causes cancer.\nIn “To Explain or to Predict”, Galit Shmueli made several important contributions to this academic debate. Interestingly, she explains why a good explanatory model can be a bad predictor due overfitting the specific sample which increases variance when applied to new data - the Bias-Variance Trade-off. Predictive models, in contrast, prioritize minimizing a combination of bias and variance, and accept a little bias to reduce that variance, meaning that the addition of a “wrong” variable can make prediction more stable.\nShe outlines several useful uses of prediction to enhance explanatory work, including hypothesis generation, determining which way of measuring a concept is the most appropriate, improving explanations and theory comparison. She recommends using predictive models as a ceiling, first running the black box algorithm to see the maximum possible accuracy and then run the theory-based model. The gap (or lack of) between the two models generates important feedback as to whether the theory captures the phenomenon a researcher is trying to measure.\nTowards a third culture\nSince Breiman and Shmueli’s papers, a potential “third culture” has emerged which marries the strengths of each school of thought. This approach - Causal Machine Learning - uses predictive algorithms inside explanatory methods, combining the accuracy benefits of the algorithmic modeling culture to strengthen the scientific claims of the data modeling culture. For example, methods like Synthetic Control have a goal which is firmly rooted in explanatory, causal inference, but now employ predictive algorithms as the means to optimize the method - e.g. using Random Forests to perfectly weight the “control states” which serves as the counterfactual to the treated unit."
  },
  {
    "objectID": "Assignments.html#reflection-2",
    "href": "Assignments.html#reflection-2",
    "title": "Knowledge Mining",
    "section": "Reflection 2",
    "text": "Reflection 2\nAI and originality\n\nWhether AI can create original content wholly depends on the subjective perception of what original output is. AI is trained on a certain body of training material, meaning that it cannot make predictions or generate content from existing knowledge outside of that database. AI, especially LLMs, are simply sophisticated prediction tools or algorithms designed to achieve a certain goal, this might lead some to conclude that AI can never truly produce truly original content and knowledge.\nHowever, at the very least, I believe that AI can accelerate the the production of original knowledge by speeding up existing research tasks. It can find papers that I’ve not been able to in Google Scholar, and when I compared myself to two LLMs at a hand-coding task (dating democratic backsliding events from around the world), it’s ability to search for information across languages across the world far out-performed mine.\nIndeed, I believe AI tools are making it possible to extract and synthesize knowledge that exists but is not currently usable. For example, I’m considering a project which would take digitized historic records of international agencies and extracting data relating to state-exits. This is something that would take hundreds of human coders hundreds of hours to do, and is now something I’m potentially able to do as a research project.\nI personally believe that recombining existing knowledge in new ways can lead to original output, so I would argue that responsible, supervised AI use can absolutely generate new knowledge.\n\nWhat is AGI?\n\nArtificial General Intelligence is a hypothesized future form of AI that exceeds human capabilities across nearly all tasks and has revolutionary implications for scientific research. It assumes future AI models will have perfect data and perfect prediction.\nAI is already out-performing medical professionals at diagnosing diseases and can pass the US bar exam. Academics (both faculty and students) are using it to improve literature reviews, analyze data and help write grant reports.\nAn AGI that can take all human knowledge, identify the most important research questions, analyze existing literature, download, clean and analyze data, and write research papers will fundamentally shift the role of a researcher. More importance will have to be placed on soft-skills, ethics and guiding the AI to benefit social goals.\nWe might see a rise in humans focusing on survey and human-subject research projects, helping the AI prioritize which questions to pursue to benefit society and translating research to policy makers."
  },
  {
    "objectID": "Lab02.html#teds2016-dataset-questions",
    "href": "Lab02.html#teds2016-dataset-questions",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Problems with the dataset\n\n\n# Define the URL\nurl &lt;- \"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\"\n\n# Create a temporary file path\ntmp &lt;- tempfile(fileext = \".dta\")\n\n# Download the file to that path\ndownload.file(url, tmp, mode = \"wb\")\n\n# Read the file using haven\nlibrary(haven)\nTEDS_2016 &lt;- read_dta(tmp)\n\n#Check structure\nstr(TEDS_2016$Tondu)\n\n dbl+lbl [1:1690] 3, 5, 3, 5, 9, 4, 9, 6, 9, 9, 5, 5, 9, 5, 4, 9, 9, 4, 3, ...\n @ label       : chr \"Position on unification and independence\"\n @ format.stata: chr \"%10.0g\"\n @ labels      : Named num [1:7] 1 2 3 4 5 6 9\n  ..- attr(*, \"names\")= chr [1:7] \"Immediate unification\" \"Maintain the status quo,move toward unification\" \"Maintain the status quo, decide either unification or independence\" \"Maintain the status quo forever\" ...\n\n# Look first few columns\nsummary(TEDS_2016[, 1:5])\n\n    District         Sex             Age           Edu            Arear      \n Min.   : 201   Min.   :1.000   Min.   :1.0   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1401   1st Qu.:1.000   1st Qu.:2.0   1st Qu.:2.000   1st Qu.:1.000  \n Median :6406   Median :1.000   Median :3.0   Median :3.000   Median :3.000  \n Mean   :4661   Mean   :1.486   Mean   :3.3   Mean   :3.334   Mean   :2.744  \n 3rd Qu.:6604   3rd Qu.:2.000   3rd Qu.:5.0   3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :6806   Max.   :2.000   Max.   :5.0   Max.   :9.000   Max.   :6.000  \n\n# View the labels associated with values\nlibrary(labelled)\n\nWarning: package 'labelled' was built under R version 4.4.1\n\nval_labels(TEDS_2016$Tondu)\n\n                                             Immediate unification \n                                                                 1 \n                   Maintain the status quo,move toward unification \n                                                                 2 \nMaintain the status quo, decide either unification or independence \n                                                                 3 \n                                   Maintain the status quo forever \n                                                                 4 \n                  Maintain the status quo,move toward independence \n                                                                 5 \n                                            Immediate independence \n                                                                 6 \n                                                       Nonresponse \n                                                                 9 \n\n#Column Names\ncolnames(TEDS_2016)\n\n [1] \"District\"        \"Sex\"             \"Age\"             \"Edu\"            \n [5] \"Arear\"           \"Career\"          \"Career8\"         \"Ethnic\"         \n [9] \"Party\"           \"PartyID\"         \"Tondu\"           \"Tondu3\"         \n[13] \"nI2\"             \"votetsai\"        \"green\"           \"votetsai_nm\"    \n[17] \"votetsai_all\"    \"Independence\"    \"Unification\"     \"sq\"             \n[21] \"Taiwanese\"       \"edu\"             \"female\"          \"whitecollar\"    \n[25] \"lowincome\"       \"income\"          \"income_nm\"       \"age\"            \n[29] \"KMT\"             \"DPP\"             \"npp\"             \"noparty\"        \n[33] \"pfp\"             \"South\"           \"north\"           \"Minnan_father\"  \n[37] \"Mainland_father\" \"Econ_worse\"      \"Inequality\"      \"inequality5\"    \n[41] \"econworse5\"      \"Govt_for_public\" \"pubwelf5\"        \"Govt_dont_care\" \n[45] \"highincome\"      \"votekmt\"         \"votekmt_nm\"      \"Blue\"           \n[49] \"Green\"           \"No_Party\"        \"voteblue\"        \"voteblue_nm\"    \n[53] \"votedpp_1\"       \"votekmt_1\"      \n\n\nThe dataset has a few issues including: - Nonresponses to survey questions were assigned a value of 9 which will artificially inflate means etc.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nTEDS_clean &lt;- TEDS_2016 %&gt;%\n  mutate(across(everything(), ~na_if(., 9))) %&gt;%\n  mutate(across(everything(), ~na_if(., 95))) %&gt;%\n  mutate(across(everything(), ~na_if(., 98)))\n\n-Variable labels should be recoded as a factor\n\nTEDS_ready &lt;- haven::as_factor(TEDS_clean)\n\n#Check it worked\nsummary(TEDS_clean$Tondu)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   3.000   4.000   3.751   5.000   6.000     121 \n\n\n\nHow to deal with missing values\n\nI’ve already recoded non-responses, originally recorded as 9 above. Next I’ll check for other nas in the original dataset (before I cleaned the nonresponses coded as 9) and get the dataset to record na if there are values of . or 9.\n\ncolSums(is.na(TEDS_2016))\n\n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2        votetsai           green \n              0               0               0             429               0 \n    votetsai_nm    votetsai_all    Independence     Unification              sq \n            429             248               0               0               0 \n      Taiwanese             edu          female     whitecollar       lowincome \n              0              10               0               0               0 \n         income       income_nm             age             KMT             DPP \n              0             330               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care      highincome \n              0               0               0               0             330 \n        votekmt      votekmt_nm            Blue           Green        No_Party \n              0             429               0               0               0 \n       voteblue     voteblue_nm       votedpp_1       votekmt_1 \n              0             429             187             187 \n\nlibrary(dplyr)\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  mutate(across(everything(), ~na_if(., 9)))\n\n\nExplore the relationship between Tondu and other variables including female, DPP, age, income, edu, Taiwanese and Econ_worse. What methods would you use?\n\nFirst I would check the nature of the variables - are they continuous or binary?\nThen I would visualize relationships and check correlations/regressions.\n\n# Check variable types\nTEDS_2016 %&gt;% \n  select(Tondu, female, DPP, age, income, edu, Taiwanese, Econ_worse) %&gt;% \n  str()\n\ntibble [1,690 × 8] (S3: tbl_df/tbl/data.frame)\n $ Tondu     : dbl+lbl [1:1690]  3,  5,  3,  5, NA,  4, NA,  6, NA, NA,  5,  5, NA,  ...\n   ..@ label       : chr \"Position on unification and independence\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"Immediate unification\" \"Maintain the status quo,move toward unification\" \"Maintain the status quo, decide either unification or independence\" \"Maintain the status quo forever\" ...\n $ female    : num [1:1690] 1 1 0 0 1 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ DPP       : num [1:1690] 0 0 0 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ age       : num [1:1690] 59 39 63 55 76 64 75 54 64 59 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income    : num [1:1690] 8 7 8 5 5.5 NA 1 10 2 5.5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ edu       : num [1:1690] 4 5 5 2 1 2 1 5 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Taiwanese : num [1:1690] 1 0 0 1 0 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Econ_worse: num [1:1690] 0 0 1 1 0 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n\n\nTondu is an ordinal variable coded from Immediate Unification through to Immediate Independence. I will recode it into numbers where immediate unification = 1 and immediate independence = 6.\n\nlibrary(dplyr)\n\nTEDS_ready &lt;- TEDS_ready %&gt;%\n  mutate(Tondu_numeric = case_when(\n    as_factor(Tondu) == \"Immediate unification\" ~ 1,\n    as_factor(Tondu) == \"Maintain the status quo,move toward unification\" ~ 2,\n    as_factor(Tondu) == \"Maintain the status quo, decide either unification or independence\" ~ 3,\n    as_factor(Tondu) == \"Maintain the status quo forever\" ~ 4,\n    as_factor(Tondu) == \"Maintain the status quo,move toward independence\" ~ 5,\n    as_factor(Tondu) == \"Immediate independence\" ~ 6,\n    TRUE ~ NA_real_  # This handles the \"Nonresponse\" or NA values\n  ))\n\nFemale is a binary variable coded 1 if respondent is female. Likewise DPP, Taiwanese and Econ_worse are also binary and likely coded 1 if the respondent supports the DPP, is Taiwanese and thinks the economy is worse.\nAge is a ratio variable and income is likely a coded scale from 1-10.\nDescriptive statistics\n\nTEDS_summary &lt;- TEDS_2016 %&gt;%\n  group_by(as_factor(Tondu)) %&gt;%\n  summarize(across(\n    c(female, DPP, age, income, edu, Taiwanese, Econ_worse), \n    ~mean(.x, na.rm = TRUE)\n  ))\nprint(TEDS_summary)\n\n# A tibble: 7 × 8\n  `as_factor(Tondu)`       female    DPP   age income   edu Taiwanese Econ_worse\n  &lt;fct&gt;                     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Immediate unification     0.519 0.0370  57.9   4.02  2.31     0.407      0.481\n2 Maintain the status quo…  0.344 0.183   55.7   5.21  3.25     0.267      0.567\n3 Maintain the status quo…  0.460 0.308   45.8   5.35  3.58     0.553      0.562\n4 Maintain the status quo…  0.540 0.220   52.8   4.94  3.03     0.585      0.521\n5 Maintain the status quo…  0.458 0.621   42.4   5.39  3.83     0.871      0.592\n6 Immediate independence    0.5   0.648   48.0   4.41  2.86     0.963      0.639\n7 &lt;NA&gt;                      0.744 0.0909  64.5   3.95  1.72     0.595      0.413\n\n\nVisualizations\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'Auto':\n\n    mpg\n\nlibrary(dplyr)\n\n#For categorical variables - female, DPP, Econ_Worse, Taiwanese:\n\n#Female\nTEDS_2016 %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(female), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#DPP\nTEDS_2016 %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(DPP), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#Econ_worse\nTEDS_2016 %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(Econ_worse), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#Taiwanese\nTEDS_2016 %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(Taiwanese), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#For continuous or ordinal - age, education, income\n# Age Plot\nggplot(TEDS_2016, aes(x = as_factor(Tondu), y = age)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n#Edu\n# Simple Age Plot\nggplot(TEDS_2016, aes(x = as_factor(Tondu), y = edu)) +\n  geom_boxplot() +\n  coord_flip()\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n#Income\n# Simple Age Plot\nggplot(TEDS_2016, aes(x = as_factor(Tondu), y = income)) +\n  geom_boxplot() +\n  coord_flip()\n\nWarning: Removed 107 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nCorrelations/regressions\n\nmodel1 &lt;- lm(Tondu_numeric ~ female + DPP + age + edu + Taiwanese + Econ_worse, data=TEDS_ready)\n\nsummary(model1)\n\n\nCall:\nlm(formula = Tondu_numeric ~ female + DPP + age + edu + Taiwanese + \n    Econ_worse, data = TEDS_ready)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8950 -0.8962 -0.0027  0.8334  3.1994 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.548854   0.176269  20.133  &lt; 2e-16 ***\nfemale       0.057622   0.054247   1.062    0.288    \nDPP          0.524508   0.059802   8.771  &lt; 2e-16 ***\nage         -0.009334   0.002091  -4.464 8.63e-06 ***\nedu         -0.008611   0.023483  -0.367    0.714    \nTaiwanese    0.757640   0.060766  12.468  &lt; 2e-16 ***\nEcon_worse  -0.031005   0.055590  -0.558    0.577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.06 on 1555 degrees of freedom\n  (128 observations deleted due to missingness)\nMultiple R-squared:  0.2098,    Adjusted R-squared:  0.2067 \nF-statistic: 68.79 on 6 and 1555 DF,  p-value: &lt; 2.2e-16\n\n# Visualize\nlibrary(sjPlot)\n\nWarning: package 'sjPlot' was built under R version 4.4.1\n\n# Visualize the coefficients\nplot_model(model1, \n           show.values = TRUE, \n           value.offset = .3,\n           title = \"Predictors of Tondu Position (Unification to Independence)\")\n\n\n\n\n\n\n\n\nThis regression plot and visualization indicates that supporting the DPP and being Taiwanese were statistically significant predictors of supporting Taiwanese independence. Being older was associated with a statistically significant decrease in support for independence, although this was a very small effect.\n\nHow about the votetsai variable (vote for DPP candidate Tsai Ing-wen)?\n\nThis suggests that voters who voted for Tsai Ing-wen were more supportive of independence and moving towards independence than voters who didn’t.\n\n# Check the labels first\nlibrary(labelled)\nval_labels(TEDS_2016$votetsai)\n\nNULL\n\n# Recode if necessary (assuming 1 is Tsai, and 0 or other numbers are missing)\nTEDS_2016 &lt;- TEDS_2016 %&gt;%\n  mutate(votetsai = na_if(votetsai, 9)) \n\nTEDS_ready %&gt;%\n  filter(!is.na(as_factor(Tondu))) %&gt;% # This removes the NA rows\n  ggplot(aes(x = as_factor(votetsai), fill = as_factor(Tondu))) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nGenerate frequency table and barchart of the Tondu variable. Assign labels to the variable using the following:\n\n\n\nTEDS_2016$Tondu&lt;-as.numeric(TEDS_2016$Tondu,labels=c(\"Unification now”, “Status quo, unif. in future”, “Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now”, “No response\"))\n\n# Standard frequency table\ntable(TEDS_2016$Tondu)\n\n\n  1   2   3   4   5   6 \n 27 180 546 328 380 108 \n\n# Percentages (better for policy reports)\nprop.table(table(TEDS_2016$Tondu)) * 100\n\n\n        1         2         3         4         5         6 \n 1.720841 11.472275 34.799235 20.905035 24.219248  6.883365"
  },
  {
    "objectID": "Assignments.html#reflection-3",
    "href": "Assignments.html#reflection-3",
    "title": "Knowledge Mining",
    "section": "Reflection 3",
    "text": "Reflection 3\nHow to start using AI for my project? Which two AI models are you using? How can you leverage them to help your research?\n\nI’m currently using AI to help accelerate the hand-coding of 706 democratic backsliding events from around the world. The dataset I’m using dates them to the month level, but I need the exact dates in order to examine how citizens responded to them in real-time via Google Search. I began hand-coding the events myself but found the AI was able to find better sources more quickly than I was.\nI’ve crafted a prompt which I give to both Gemini and Chat GPT, asking it to identify the exact date, cite sources, and determine if the event meets the definition of democratic backsliding that I’ve developed from the literature.\nI got 100 events through before I realized that I didn’t prompt the LLMs on how to prioritize the date where multiple dates were possible (e.g. the passage of legislation, versus entry into force). I asked each LLM to explain how it had been assigning the dates, and got a third LLM (Claude) to compare their reasonings.\nI then asked Claude to help me engineer a prompt to get the two LLMs to prioritize the first date the event would have become known to the public. I also asked Claude to review instances where the two LLMs disagreed on dates.\nTBC on group project,"
  },
  {
    "objectID": "Assignments.html#assignment-2",
    "href": "Assignments.html#assignment-2",
    "title": "Knowledge Mining",
    "section": "Assignment 2:",
    "text": "Assignment 2:\nReviewing the applications of data-mining in the real world:\n\nI asked three LLMs (Gemini 3, Chat GPT and Claude) to conduct a 2,000-word structured systematic literature review on data mining and machine learning applications in political science and international relations, with a specific focus on modeling state exits from international organizations (IOs). I analyzed the three outputs and asked Gemini to combine the strongest sections into a final review which I’ve shared below along with my reflections:\n\nData Mining and Machine Learning in International Relations: A Systematic Review\nIntroduction\nInternational relations (IR) research has experienced a major methodological shift over the past two decades, driven by the growth of digitized political data, new sources of real-time information, and advances in machine learning (ML) and data mining. These computational approaches have expanded what scholars can study and how they can study it—enabling analysis of complex, high-dimensional, and rapidly changing international phenomena such as conflict dynamics, alliance formation, trade and sanctions, diplomatic signaling, and institutional cooperation.\nWhere traditional quantitative IR often relied on relatively low-dimensional datasets and parametric regression models, computational IR increasingly leverages automated data generation (e.g., event data, satellite imagery, large-scale text corpora) and flexible algorithms capable of capturing nonlinearity, interaction effects, and temporal dependence. The result has been both a practical expansion of predictive capacity—especially for rare events—and a renewed debate over what constitutes “good” inference in IR: explanation, prediction, or some productive combination of both.\nThis systematic literature review synthesizes influential and representative work at the intersection of machine learning, data mining, and international relations. It documents methodological trends, major substantive applications, and ongoing debates about theory, transparency, and validation. The review highlights how computational methods have reshaped four core areas of IR: (1) conflict forecasting and political instability, (2) text-as-data and diplomatic communication, (3) event data and temporal dynamics, and (4) networks and international systems. It concludes by identifying persistent gaps and outlining promising directions for future research.\nMethodology\nReview Design\nThis review follows systematic synthesis principles adapted from PRISMA-style approaches, recognizing that computational IR is methodologically heterogeneous and spans multiple subfields. A narrative synthesis approach was used rather than meta-analysis.\nTwo selection logics were combined:\nSystematic keyword search to capture breadth across IR subdomains\nCitation-weighted screening to ensure inclusion of foundational and field-shaping contributions\n2.2 Search Strategy\nSearches were conducted across Google Scholar, Web of Science, Scopus, JSTOR, and arXiv using combinations of: “machine learning” OR “data mining” AND “international relations” “conflict prediction,” “early warning,” “political instability” “text as data,” “NLP,” “topic model,” “UN speeches,” “treaty text” “event data,” “ICEWS,” “GDELT,” “automated coding” “network analysis” AND “alliances,” “trade,” “international institutions”\nBackward and forward citation searches were used to identify connected clusters of influential work.\nInclusion Criteria\nStudies were included if they:\n\nApplied ML, computational text analysis, or data mining methods\nFocused on core IR outcomes (conflict, cooperation, diplomacy, trade, institutions, bargaining, sanctions, alliances)\nWere published primarily in the modern computational era (roughly 2000–2025)\n\nWere either highly influential (citation impact) or illustrative of emerging trends\n3. Computational IR: Foundational Debates and Methodological Shifts\n3.1 Prediction versus Explanation\nOne of the most influential methodological contributions in computational IR is the critique that conventional explanatory models often perform poorly when evaluated on predictive grounds. Ward, Greenhill, and Bakke (2010) argued that many civil war models—despite producing statistically significant results—fail out-of-sample, implying limited usefulness for forecasting or policy applications. This critique catalyzed a broader debate about the goals of modeling in IR.\nShmueli (2010) formalized the distinction between explanatory and predictive modeling, while Cranmer and Desmarais (2017) argued for complementarity: predictive modeling can improve theory by revealing nonlinearities, interactions, and boundary conditions that are often missed in parametric regression frameworks. In practice, computational IR increasingly treats prediction as (1) a separate modeling objective and (2) a validation tool that disciplines overfitting and strengthens credibility.\n3.2 Rare Events, Class Imbalance, and Evaluation\nMany key IR outcomes are rare events: conflict onset, coups, genocides, sanctions escalations, or diplomatic breakdowns. This creates class imbalance problems that weaken standard logistic regression performance. Muchlinski et al. (2016) demonstrated that ensemble methods such as random forests often outperform logistic regression for predicting civil war onset, particularly when events are infrequent.\nThe computational IR literature has consequently emphasized:\n\nHandling class imbalance (reweighting, resampling, rare-events correction)\nChoosing evaluation metrics appropriate to rare events (precision–recall, AUC-PR, calibration)\nUsing temporal and out-of-sample validation designs rather than purely in-sample fit\n\nThis methodological infrastructure has been crucial for building early warning systems and forecasting pipelines.\n4. Core Applications of Machine Learning and Data Mining in IR\n4.1 Conflict Forecasting and Political Instability\nConflict prediction is the most developed and widely cited domain of ML in IR. Researchers have used algorithms such as random forests, gradient boosting, support vector machines, and neural networks to forecast civil war onset, conflict escalation, and state instability.\nA key contribution of this literature is the development of early warning systems that shift attention from retrospective explanation to prospective risk assessment. Hegre and colleagues’ forecasting work illustrates how large datasets, demographic and economic projections, and flexible modeling can generate long-term predictions useful for policy planning.\nColaresi and Mahmood (2017) synthesized lessons from ML for conflict research, emphasizing that predictive performance depends on thoughtful feature construction, modeling choices, and appropriate validation—not merely on deploying complex algorithms. More recent critiques (e.g., Blair & Sambanis, 2020) argue that the most successful conflict forecasting work is not “atheoretical ML,” but rather ML informed by substantive knowledge and careful data construction.\nSubstantive insights from conflict forecasting:\nPolitical instability often follows measurable trajectories rather than appearing suddenly\nNonlinear relationships and interactions are common (e.g., regime type interacting with economic shocks)\nPredictive performance improves when models incorporate both structural conditions and short-run behavioral signals\n4.2 Event Data and Temporal Modeling\nA major enabler of computational IR has been the automation of political event data. Systems such as ICEWS and GDELT extract structured representations of international interactions from news sources, producing high-frequency indicators of cooperation and conflict. Earlier foundational work by King and Lowe (2003) and later advances in automated coding frameworks (e.g., Schrodt’s contributions) created the infrastructure that ML applications now rely upon.\nMachine learning methods applied to event data have been used to forecast:\n\nEscalation dynamics\nDiplomatic breakdown and crisis onset\nSanctions and retaliation patterns\nShifts in interstate cooperation\n\nThe key conceptual contribution here is dynamic modeling: international politics is treated as a time-dependent process, where recent events, momentum, and sequencing matter. This has pushed IR toward models that capture temporal dependence, regime shifts, and path dependence.\n4.3 Text-as-Data, NLP, and Diplomatic Communication\nText analysis is arguably the single most transformative computational development in political science and has been extensively adopted in IR. Grimmer and Stewart (2013) provided the foundational methodological framework for automated content analysis, emphasizing that text methods require validation, conceptual clarity, and attention to measurement error.\nRoberts et al. (2014) introduced structural topic models (STM), allowing covariates to influence topic prevalence and content. IR researchers quickly applied STM and other NLP techniques to UN General Assembly speeches, treaty texts, parliamentary debates, and media coverage.\nProminent IR applications include:\n\nExtracting state preferences and issue emphasis from UN speeches\nMeasuring rhetorical shifts in foreign policy priorities over time\nEstimating latent positions using speech and voting data (e.g., Bailey, Strezhnev & Voeten, 2017)\nStudying framing and persuasion in diplomacy and negotiation\n\nA central insight from this literature is that political language contains strategic signals. Rhetorical shifts can indicate preference change, resolve, coalition-building, or domestic audience positioning—often before behavior becomes observable in policy outputs.\nRecent developments also point toward multilingual and large language model (LLM) applications, including classification and scaling tasks across languages and contexts. While promising, this trend intensifies debates about transparency, reproducibility, and measurement validity.\n4.4 Networks and International Systems\nNetwork analysis provides a system-level lens on IR, treating international politics as embedded in webs of alliances, trade relationships, institutional ties, and diplomatic interaction. Hafner-Burton, Kahler, and Montgomery (2009) provided a foundational synthesis of network approaches in IR, demonstrating how network structure shapes influence, diffusion, and cooperation.\nNetwork methods have been applied to:\n\nAlliance systems and security partnerships\nTrade networks and economic interdependence\nInternational organizations and institutional density\nConflict diffusion and contagion\nMultiplex networks linking multiple domains (trade + alliances + diplomacy)\n\nComputational network techniques—including community detection, clustering, centrality analysis, and network evolution models—allow scholars to identify blocs, measure influence, and model how network position shapes strategic behavior.\nSubstantive insights from network IR:\n\nInfluence is often relational rather than purely attribute-based\nSystem structure affects dyadic outcomes (e.g., conflict depends on network environment)\nCooperation and conflict diffuse through networks rather than occurring independently\n\n5. Integrating ML with Causal Inference in IR\nA growing methodological frontier is the integration of predictive ML with causal inference. IR scholars increasingly recognize that prediction alone does not answer causal questions central to theory and policy.\nTwo influential toolkits include:\n\nDouble machine learning approaches that flexibly model confounding while preserving valid inference (Chernozhukov et al., 2018)\nCausal forests and heterogeneous treatment effect estimation (Athey & Imbens, 2019)\n\nThese methods are particularly relevant in IR because observational data often involve complex confounding structures (e.g., sanctions, peacekeeping, foreign aid, alliance formation). Computational causal methods offer a potential bridge between flexible modeling and interpretable causal claims—though they still require careful research design, transparent assumptions, and credible identification strategies.\n6. Persistent Challenges in Computational IR\nDespite rapid growth, the literature identifies several recurring challenges.\n6.1 Interpretability and Theory Development\nMany high-performing ML models operate as black boxes, raising concerns about whether prediction improves explanation or contributes meaningfully to theory. Feature importance metrics are often incorrectly interpreted as causal effects, and strong predictive accuracy may conceal weak theoretical engagement.\nThe best computational IR work increasingly treats ML as a complement to theory: a tool for discovering patterns, testing model generalizability, and generating new hypotheses rather than replacing substantive reasoning.\n6.2 Data Quality, Measurement, and Construct Validity\nAutomated data sources—event data, text corpora, satellite imagery—carry measurement error, selection bias, and shifting coverage over time. News-based event data, for example, may reflect media incentives and geopolitical attention rather than true underlying events. Text corpora vary in genre, strategic intent, and translation quality.\nAs a result, validation and robustness checks are central:\n\nHuman validation of text and coded events\nSensitivity to corpus choice and preprocessing decisions\nTemporal validation to address dataset drift\n\n6.3 Replication and Transparency\nComputational IR faces risks of replicability problems due to incomplete reporting of preprocessing pipelines, hyperparameters, and model selection procedures. Best practices increasingly emphasize:\n\nFull documentation of text preprocessing and feature engineering\nShared code and reproducible workflows\nPre-registration or clear separation of training and testing decisions\n\n7. Gaps and Future Directions\nSeveral promising opportunities remain underdeveloped.\n\nUnderexplored substantive domains\n\nDiplomatic bargaining, negotiation processes, institutional design, and international legal compliance remain less computationally developed than conflict prediction.\nReal-time and streaming analysis\n\nThere is increasing interest in models that track international dynamics in near real time (especially using event data and social media), but concerns about data drift and manipulation persist.\nMultimodal IR\n\nCombining text, event data, networks, and geospatial/satellite imagery remains technically demanding but conceptually promising, particularly for monitoring conflict, sanctions evasion, or humanitarian crises.\nBetter integration with qualitative and formal methods\n\nMulti-method designs—using ML to identify patterns and qualitative work to validate mechanisms—could help resolve interpretability concerns and strengthen theory-building.\n\nSuggested hypothesis:\n\nRecommended Hypothesis (Most Compelling)\nH: Latent Dissatisfaction and Behavioral Change\nStates that exhibit increasing rhetorical distance from prevailing international norms will subsequently reduce their cooperative international behavior, even before observable policy shifts occur.\nConclusion\nMachine learning and data mining have reshaped international relations by expanding the empirical terrain of the discipline and enabling models capable of handling complexity, high dimensionality, and temporal dependence. The most mature applications—conflict forecasting, automated event data, text-as-data, and network analysis—demonstrate that computational tools can uncover patterns that are difficult to detect with traditional methods and can provide meaningful improvements in predictive performance for rare and dynamic outcomes.\nAt the same time, computational IR continues to confront enduring challenges: balancing prediction with explanation, ensuring construct validity, addressing bias and drift in automated data sources, and improving transparency and replicability. The most productive path forward is not algorithmic sophistication alone, but deeper integration of computational methods with IR theory, research design, and validation norms.\nAs global politics becomes increasingly data-rich and fast-moving, computational methods are likely to become central tools in IR scholarship—especially where they are used not as substitutes for theory, but as instruments for improving measurement, testing generalizability, and generating new theoretical insight.\nSelected References (indicative core)\nAthey, S., & Imbens, G. (2019).\n\nBailey, M., Strezhnev, A., & Voeten, E. (2017).\n\nChernozhukov, V. et al. (2018).\n\nColaresi, M., & Mahmood, Z. (2017).\n\nCranmer, S., & Desmarais, B. (2017).\n\nGrimmer, J., & Stewart, B. (2013).\n\nHafner-Burton, E., Kahler, M., & Montgomery, A. (2009).\n\nHegre, H. et al. (2017).\n\nKing, G., & Lowe, W. (2003).\n\nMuchlinski, D. et al. (2016).\n\nRoberts, M. et al. (2014).\n\nShmueli, G. (2010).\n\nWard, M., Greenhill, B., & Bakke, K. (2010).\n\nReflection:\n\nHow did each model approach the systematic review differently?\nGemini\n\nGemini showed me some limited methodology, including the databases it looked at, how it categorized the papers by method and some of the search terms it used. It didn’t show me any sources or references.\nIt also over-prioritized inserting what it knows about me into its output even when it’s not relevant. I’ve noticed it does this with other queries as well - e.g. mentioning I’m in graduate school when asking it for a recipe or running advice.\n\nChat GPT\n\nSet out it’s search strategy and inclusion criteria, giving me the exact words and combinations it used to search the databases for which boosted my confidence in its output. It also told me how many papers it had found and provided a selected reference list at the end, I checked some of them and they were real.\nI genuinely learned a lot from its original output about how machine learning and knowledge mining had been used in IR more broadly, and in my specific interest area (why states leave international organizations). I learned that it seems to have primarily been used to do text-based analysis and network analysis.\nI do suspect that it also took things it knows about me when it generated the gaps section, as these were specific to one of my ongoing projects.\n\nClaude\n\nClaude gave me the most detailed insights into its methodology, listing the databases it searched, the date-range it looked at and the search terms and combinations it used. It also shared its inclusion and exclusion criteria and gave me a very detailed breakdown on how it found 1,847 articles and decided to include 87 of those in the study. It did not provide cititations or a reference list.\nIt’s overview of which ML techniques are used in political science and IR was very helpful, and similarly to GPT identified that text-as-data/sentiment analysis, and network analysis were popular approaches. It identified that deep learning applications remained relatively rare.\nI wasn’t aware of any studies that had used machine learning tools on my topic area of interest (exit from international organizations), yet Claude claimed to have identified eight. I challenged this and they turned out not to be true.\nClaude also consistently ignored the instruction that the literature review should be 2000 words.\n\nWhich prompt refinements yielded the best results for each model?\nAsking Claude to provide in text citations and a reference list enabled me to check the content it was sharing with me. Especially the content which mentioned the eight papers which had used ML tools to look at state exits from IOs. When I challenged it on the existence of these papers it admitted that: “You’ve caught an important issue. In this systematic literature review, I constructed a realistic academic narrative to demonstrate the structure and style of a systematic review, but I did not actually conduct the review or have access to eight specific studies that used ML techniques to model IO withdrawals.”!!\nI’m not aware of any ML tools that have been used and decided to prompt the other two LLMs more broadly to refine their literature review on applications within international relations more broadly.\nAsking Gemini to reconduct the review in greater depth while looking at IR more broadly, give in-text citations and a fully reference list improved its output a lot.\nI also asked GPT to re-issue the output and identify the key papers in IR that used machine learning and to define “key” and most-cited.\n\nWhat did you learn about leveraging AI for structured academic reviews?\nI learned that asking the LLM to define key papers by number of citations helps to identify the really pivotal papers on the topic.\nI also became aware of the need to closely read and challenge the output when it doesn’t look right, and to ask for citations and references to check for hallucinations.\nI found it very laborious to ask, manage and challenge three LLMs to generate the literature review. I’d rather use a main one plus other LLMs to explain concepts that aren’t clear or review the other LLMs output, rather than generating three outputs and having to figure out the strengths and weaknesses of all of them.\nI’ve mainly been using Gemini but the poor quality of its output convinced me to go back to GPT for academic use."
  }
]